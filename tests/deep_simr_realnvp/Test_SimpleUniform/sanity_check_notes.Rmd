---

title: "Normalizing Flows Sanity Checks: Gaussianity & Decorrelation"
author: ""
date: "`r format(Sys.Date())`"
output:
html\_document:
toc: true
toc\_depth: 3
toc\_float: true
df\_print: paged
----------------

# Overview

This report summarizes sanity checks for a RealNVP-style normalizing flow on a simple synthetic dataset (uniform 10,000×4). We assess whether the learned transformation achieves (a) **approximately Gaussian marginals** and (b) **more balanced/less ill‑conditioned covariance** (decorrelation/isotropization). We also document the issues we saw **before** adjusting training hyperparameters, explain **why** those issues occurred, and show the **improvements** after the changes.

## TL;DR

* Early runs produced pathologies (e.g., sharp spikes near 0 in \$z\$, or nearly unchanged/uniform \$z\$), indicating optimization/parameterization trouble.
* After adjusting base variance, normalization cadence, coupling structure, and selection metric, the flow behaved as expected:

  * Raw uniforms → bell‑shaped \$z\$ marginals (lower |skew|, lower |excess kurtosis|).
  * Covariance conditioning improved (lower condition number; smaller PC1 variance share).
  * Some off‑diagonal correlation appeared in \$z\$ (expected without explicit independence penalties).

# Reproducibility & Inputs

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(readr)
library(scales)
library(patchwork)
library(RColorBrewer)

# ---- Parameters ----
raw_path <- "UniformSimulatedData/uniform_10000x4.csv"       # raw inputs (uniform)
z_path   <- "runs/uniform_z_view0.csv"                       # transformed features (z)
outdir   <- "sanity_uniform_report"

if (!dir.exists(outdir)) dir.create(outdir, recursive = TRUE)

# Helper: safe matrix loader
load_numeric_matrix <- function(path) {
  stopifnot(file.exists(path))
  m <- suppressMessages(readr::read_csv(path, show_col_types = FALSE))
  m <- as.matrix(m)
  storage.mode(m) <- "double"
  m
}

raw <- load_numeric_matrix(raw_path)
z   <- load_numeric_matrix(z_path)

stopifnot(nrow(raw) == nrow(z), ncol(raw) == ncol(z))

n <- nrow(raw); p <- ncol(raw)
cat(sprintf("Loaded raw %d×%d and z %d×%d\n", n, p, nrow(z), ncol(z)))
```

# What went wrong initially (diagnosis)

**Symptoms we saw:**

* \$z\$ histograms with **needle‑like spikes at 0** (extremely concentrated probability mass),
* or \$z\$ histograms that remained **nearly uniform**,
* and little improvement in covariance conditioning.

**Likely causes:**

1. **Base distribution too tight:** A very small `base-sigma` can freeze scales/actnorm early; gradients focus on log‑det terms rather than shaping marginals.
2. **Insufficient normalization cadence:** Infrequent or delayed actnorm can lead to unstable scales and saturating couplings.
3. **Under‑expressive early couplings:** Too few additive channels at the beginning can make learning brittle on simple data.
4. **Overly aggressive scale parameters:** Large `scale-cap` or missing spectral normalization can cause exploding or vanishing transforms.
5. **Selection metric:** Using a smoothed surrogate instead of true `val_bpd` occasionally favored degenerate optima.

**Parameter changes we adopted:**

* `--base-distribution DiagGaussian --base-sigma 1.0`
* `--K 8 --scale-cap 1.5 --spectral-norm-scales`
* `--additive-first-n 4 --actnorm-every 1 --mask-mode rolling`
* `--lr 2e-4 --weight-decay 1e-5`
* `--best-selection-metric val_bpd`

After these changes, the training produced sensible \$z\$.

# Sanity check 1 — Gaussianity of marginals

We quantify Gaussianity via **mean absolute skewness**, **mean absolute excess kurtosis**, and a **(optional) KS test pass fraction** against \$\mathcal{N}(0,1)\$ after per‑feature standardization.

```{r gaussianity-metrics}
# zscore function
zscore <- function(m) {
  mcenter <- sweep(m, 2, colMeans(m, na.rm = TRUE), FUN = "-")
  msd <- sqrt(colMeans(mcenter^2, na.rm = TRUE))
  sweep(mcenter, 2, pmax(msd, .Machine$double.eps), FUN = "/")
}

skew <- function(x) {
  x <- x[is.finite(x)]
  m <- mean(x); s <- sd(x)
  if (s == 0) return(0)
  mean(((x - m)/s)^3)
}

excess_kurt <- function(x) {
  x <- x[is.finite(x)]
  m <- mean(x); s <- sd(x)
  if (s == 0) return(0)
  mean(((x - m)/s)^4) - 3
}

summ_gauss <- function(m, label) {
  mz <- zscore(m)
  tibble(
    label = label,
    mean_abs_skew   = mean(abs(apply(mz, 2, skew))),
    mean_abs_exkurt = mean(abs(apply(mz, 2, excess_kurt)))
  )
}

gauss_tbl <- bind_rows(
  summ_gauss(raw, "raw"),
  summ_gauss(z,   "z")
)

gauss_tbl
```

```{r gaussianity-plot, fig.width=12, fig.height=4}
p1 <- ggplot(gauss_tbl, aes(label, mean_abs_skew)) +
  geom_col(width = 0.6, fill = "#ff786e") +
  labs(title = "Mean |skew| (lower better)", x = NULL, y = NULL)

p2 <- ggplot(gauss_tbl, aes(label, mean_abs_exkurt)) +
  geom_col(width = 0.6, fill = "#ff786e") +
  labs(title = "Mean |excess kurtosis| (lower better)", x = NULL, y = NULL)

p1 + p2
```

**Expectation:** Raw uniforms have near‑zero skew but **strong negative excess kurtosis** (platykurtic). A well‑trained flow should yield \$z\$ with **lower |excess kurtosis|** (closer to 0) and small skew.

# Sanity check 2 — Decorrelation / conditioning

We examine (i) **mean absolute off‑diagonal correlation**, (ii) **covariance matrix condition number**, and (iii) **PC1 variance fraction**.

```{r decor-metrics}
mean_abs_offdiag_corr <- function(m) {
  cmat <- cor(m)
  msk <- row(cmat) != col(cmat)
  mean(abs(cmat[msk]))
}

cond_number <- function(m) {
  s <- svd(cov(m))$d
  max(s) / pmax(min(s), .Machine$double.eps)
}

pc1_var_frac <- function(m) {
  s <- svd(cov(m))$d
  s[1] / sum(s)
}

summ_decor <- function(m, label) {
  tibble(
    label = label,
    mean_abs_corr = mean_abs_offdiag_corr(m),
    cond_number   = cond_number(m),
    pc1_var_frac  = pc1_var_frac(m)
  )
}

decor_tbl <- bind_rows(
  summ_decor(raw, "raw"),
  summ_decor(z,   "z")
)

decor_tbl
```

```{r decor-plots, fig.width=12, fig.height=4}
p3 <- ggplot(decor_tbl, aes(label, mean_abs_corr)) +
  geom_col(width = 0.6, fill = "#ff786e") +
  labs(title = "Mean |corr| offdiag (lower better)", x = NULL, y = NULL)

p4 <- ggplot(decor_tbl, aes(label, cond_number)) +
  geom_col(width = 0.6, fill = "#ff786e") +
  scale_y_log10(labels = label_number(accuracy = 1)) +
  labs(title = "Cov condition number (lower better)", x = NULL, y = NULL)

p5 <- ggplot(decor_tbl, aes(label, pc1_var_frac)) +
  geom_col(width = 0.6, fill = "#ff786e") +
  labs(title = "PC1 variance fraction (lower better)", x = NULL, y = NULL)

(p3 | p4 | p5)
```

**Interpretation:**

* The **condition number** and **PC1 fraction** should drop in \$z\$ if the flow spreads variance more evenly (toward isotropy).
* **Mean |corr|** may **increase** in \$z\$ unless we explicitly penalize dependence; flows optimize likelihood, not independence per se.

# Visual sanity: Histograms

```{r histograms, fig.width=13, fig.height=4}
plot_hist <- function(m, title_prefix) {
  df <- as_tibble(m) |> setNames(paste0("V", seq_len(ncol(m)))) |> mutate(row = row_number())
  p_list <- lapply(names(df)[names(df) != "row"], function(v) {
    ggplot(df, aes(.data[[v]])) +
      geom_histogram(bins = 60, fill = "#6ea3ff", color = "white", linewidth = 0.2) +
      labs(title = v, x = NULL, y = NULL)
  })
  wrap_plots(p_list) + plot_annotation(title = title_prefix)
}

plot_hist(raw, "Raw uniforms")
plot_hist(z,   "Transformed z")
```

# Results summary

```{r knit-summary, echo=FALSE}
knitr::kable(gauss_tbl, caption = "Gaussianity metrics")
knitr::kable(decor_tbl, caption = "Decorrelation/conditioning metrics")
```

* **Gaussianity**: \$z\$ features show substantially reduced |excess kurtosis| and modest skew → closer to normal.
* **Conditioning**: Covariance condition number and PC1 variance share decrease → more balanced variance.
* **Correlations**: Off‑diagonals in \$z\$ rise vs. raw (expected without independence constraints).

# Why the changes helped

* A **larger base sigma** avoided overly tight priors that ‘pin’ scales and suppress gradient signal.
* **Actnorm every layer** stabilized activations and improved gradient flow.
* **More additive channels early** gave the network an easy path to shape marginals before learning aggressive scalings.
* **Spectral norm + scale cap** tamed pathological growth in \$\log s\$ and prevented numeric blow‑ups.
* Selecting by **`val_bpd`** aligned model choice with the true objective (likelihood), avoiding artifacts favored by smoothed proxies.

# Practical guidance & next steps

1. **Use these sanity plots for any new dataset** to confirm the flow is shaping marginals and variance as intended.
2. If independence matters for downstream linear models, consider:

   * adding a **decorrelation penalty** (e.g., mean |corr| on a validation batch),
   * **whitening in \$z\$** post‑hoc (PCA/ZCA on training set) before fitting linear models,
   * or experimenting with **whitening‑aware coupling layers**.
3. Track during training: mean |skew|, mean |excess kurtosis|, condition number — falling curves indicate healthy learning.

# Session info

```{r}
sessionInfo()
```
